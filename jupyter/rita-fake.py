#!/usr/bin/env python
# coding: utf-8

# In[2]:


from elasticsearch import Elasticsearch
from elasticsearch.dsl import Search
import pandas as pd
import numpy as np
import urllib3
from datetime import datetime, timedelta
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
from pprint import pprint
import json


# In[3]:


# Thi·∫øt l·∫≠p k·∫øt n·ªëi ƒë·∫øn Elasticsearch
# conn = Elasticsearch(
#     ['https://192.168.145.101:9200'], 
#     ca_certs=False, 
#     verify_certs=False,
#     basic_auth=('jupyter', 'jupyter@seconi.com'),
# )

# print(conn.info())


# In[ ]:


# print(conn.indices.get_data_stream(name="logs-zeek*"))


# In[5]:


# zlog = Search(using=conn, index='logs-zeek-so')

# conn_log = zlog.query("term", **{"event.dataset": "zeek.conn"})
# http_log = zlog.query("term", **{"event.dataset": "zeek.http"})
# dns_log = zlog.query("term", **{"event.dataset": "zeek.dns"})
# ssl_log = zlog.query("term", **{"event.dataset": "zeek.ssl"})

# conn_12h = conn_log.filter('range', **{'@timestamp': {'gte': 'now-12h', 'lt': 'now'}})
# http_12h = http_log.filter('range', **{'@timestamp': {'gte': 'now-12h', 'lt': 'now'}})
# dns_12h = dns_log.filter('range', **{'@timestamp': {'gte': 'now-12h', 'lt': 'now'}})
# ssl_12h = ssl_log.filter('range', **{'@timestamp': {'gte': 'now-12h', 'lt': 'now'}})


# In[6]:


conn_df = pd.DataFrame()
# for hit in conn_12h.scan():
#     data = hit.to_dict()['message']
#     data = json.loads(data)
#     row = pd.DataFrame([data])
#     conn_df = pd.concat([conn_df, row], ignore_index=True)

http_df = pd.DataFrame()
# for hit in http_12h.scan():
#     data = hit.to_dict()['message']
#     data = json.loads(data)
#     row = pd.DataFrame([data])
#     http_df = pd.concat([http_df, row], ignore_index=True)

dns_df = pd.DataFrame()
# for hit in dns_12h.scan():
#     data = hit.to_dict()['message']
#     data = json.loads(data)
#     row = pd.DataFrame([data])
#     dns_df = pd.concat([dns_df, row], ignore_index=True)

ssl_df = pd.DataFrame()
# for hit in ssl_12h.scan():
#     data = hit.to_dict()['message']
#     data = json.loads(data)
#     row = pd.DataFrame([data])
#     ssl_df = pd.concat([ssl_df, row], ignore_index=True)


# In[ ]:


# hashmap = {}
# # --- CONN LOG ---
# conn_grouped = conn_df.groupby(['id.orig_h', 'id.resp_h'])
# conn_df_result = {}
# for name, group in conn_grouped:
#     h = hash(name)
#     conn_df_result[h] = {
#         "tsList": group["ts"].tolist(),
#         "byteList": group["orig_ip_bytes"].tolist()
#     }
#     hashmap[h] = name   # l∆∞u l·∫°i tuple g·ªëc

# # --- SSL LOG ---
# ssl_grouped = ssl_df.groupby(['id.orig_h', 'server_name'])
# ssl_result = {}
# for name, group in ssl_grouped:
#     h = hash(name)
#     ssl_result[h] = {
#         "tsList": group["ts"].tolist()
#     }
#     hashmap[h] = name

# # --- HTTP LOG ---
# http_grouped = http_df.groupby(['id.orig_h', 'host'])
# http_result = {}
# for name, group in http_grouped:
#     # get 
#     h = hash(name)
#     http_result[h] = {
#         "ts": group["ts"].tolist()
#     }
#     hashmap[h] = name

# # --- DNS LOG ---
# dns_grouped = dns_df.groupby(['id.orig_h', 'id.resp_h', 'query'])
# dns_result = {}
# for name, group in dns_grouped:
#     h = hash(name)
#     dns_result[h] = {
#         "ts": group["ts"].tolist()
#     }
#     hashmap[h] = name

# full_dict_log = conn_df_result | ssl_result | http_result | dns_result

# grouped_df = pd.DataFrame.from_dict(full_dict_log, orient='index')


# ==================== Ver2 ========================
conn_df = pd.read_csv('zeek_conn_12h.csv')
http_df = pd.read_csv('zeek_http_12h.csv')
dns_df = pd.read_csv('zeek_dns_12h.csv')
ssl_df = pd.read_csv('zeek_ssl_12h.csv')
import pandas as pd

def aggregate_with_log_source(conn_df: pd.DataFrame, http_df: pd.DataFrame, dns_df: pd.DataFrame, ssl_df: pd.DataFrame) -> (pd.DataFrame, dict):
    """
    Aggregate c√°c log ·ª©ng d·ª•ng v√† l√†m gi√†u k·∫øt qu·∫£ v·ªõi byteList v√† log_source.

    Args:
        conn_df: DataFrame t·ª´ conn.log.
        http_df: DataFrame t·ª´ http.log.
        dns_df: DataFrame t·ª´ dns.log.
        ssl_df: DataFrame t·ª´ ssl.log.

    Returns:
        M·ªôt tuple ch·ª©a:
        - DataFrame t·ªïng h·ª£p ƒë√£ ƒë∆∞·ª£c l√†m gi√†u.
        - Hashmap ƒë·ªÉ tra c·ª©u ng∆∞·ª£c t·ª´ hash ra key g·ªëc.
    """
    
    # --- B∆∞·ªõc 1: T·∫°o map ƒë·ªÉ tra c·ª©u nhanh t·ª´ uid -> orig_ip_bytes ---
    uid_to_bytes_map = conn_df[['uid', 'orig_ip_bytes']].drop_duplicates('uid').set_index('uid')['orig_ip_bytes']
    full_dict_log = {}
    hashmap = {}

    # --- SSL LOG ---
    ssl_grouped = ssl_df.groupby(['id.orig_h', 'server_name'])
    for name, group in ssl_grouped:
        h = hash(name)
        # D√πng .map() ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
        byte_list = group['uid'].map(uid_to_bytes_map).tolist()
        full_dict_log[h] = {
            "log_source": "ssl",
            "tsList": group["ts"].tolist(),
            "byteList": byte_list
        }
        hashmap[h] = name

    # --- HTTP LOG ---
    http_grouped = http_df.groupby(['id.orig_h', 'host'])
    for name, group in http_grouped:
        h = hash(name)
        # D√πng .map() ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
        byte_list = group['uid'].map(uid_to_bytes_map).tolist()
        full_dict_log[h] = {
            "log_source": "http",
            "tsList": group["ts"].tolist(),
            "byteList": byte_list
        }
        hashmap[h] = name
        
    # --- DNS LOG ---
    dns_grouped = dns_df.groupby(['id.orig_h', 'id.resp_h', 'query'])
    for name, group in dns_grouped:
        h = hash(name)
        # D√πng .map() ƒë·ªÉ ƒë·∫£m b·∫£o th·ª© t·ª±
        byte_list = group['uid'].map(uid_to_bytes_map).tolist()
        full_dict_log[h] = {
            "log_source": "dns",
            "tsList": group["ts"].tolist(),
            "byteList": byte_list
        }
        hashmap[h] = name

    final_df = pd.DataFrame.from_dict(full_dict_log, orient='index')
    if not final_df.empty:
        final_df = final_df[['log_source', 'tsList', 'byteList']]
    
    return final_df, hashmap

# --- C√ÅCH S·ª¨ D·ª§NG ---

# G·ªçi h√†m
final_df, hashmap = aggregate_with_log_source(conn_df, http_df, dns_df, ssl_df)

# In k·∫øt qu·∫£
# print("--- Final DataFrame (with log_source only) ---")
# print(final_df)

# Tra c·ª©u ng∆∞·ª£c v·∫´n h·ªØu √≠ch
if not final_df.empty:
    second_hash_key = final_df.index[1]
    original_key = hashmap[second_hash_key]
    print(f"\n--- Reverse Lookup Example ---")
    print(f"Hash '{second_hash_key}' corresponds to the original key: {original_key}")

def test_aggregation_correctness(
    final_df: pd.DataFrame, 
    hashmap: dict,
    conn_df: pd.DataFrame, 
    http_df: pd.DataFrame, 
    dns_df: pd.DataFrame, 
    ssl_df: pd.DataFrame
):
    """
    Ki·ªÉm tra xem final_df c√≥ ƒë∆∞·ª£c t·ªïng h·ª£p ch√≠nh x√°c t·ª´ c√°c log g·ªëc kh√¥ng.
    - Ki·ªÉm tra s·ª± to√†n v·∫πn v·ªÅ s·ªë l∆∞·ª£ng (length).
    - Ki·ªÉm tra s·ª± t∆∞∆°ng ·ª©ng v√† ch√≠nh x√°c c·ªßa tsList v√† byteList.
    """
    print("--- B·∫Øt ƒë·∫ßu ki·ªÉm tra t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa d·ªØ li·ªáu ---")
    
    # C·∫•u h√¨nh ƒë·ªÉ d·ªÖ d√†ng truy c·∫≠p v√†o DataFrame v√† c√°c c·ªôt g·ªëc
    source_map = {
        "http": http_df,
        "dns": dns_df,
        "ssl": ssl_df
    }
    group_cols_map = {
        "http": ['id.orig_h', 'host'],
        "dns": ['id.orig_h', 'id.resp_h', 'query'],
        "ssl": ['id.orig_h', 'server_name']
    }
    
    # T·∫°o l·∫°i uid_to_bytes_map l√†m "ngu·ªìn ch√¢n l√Ω" (source of truth)
    uid_to_bytes_map = conn_df[['uid', 'orig_ip_bytes']].drop_duplicates('uid').set_index('uid')['orig_ip_bytes']

    passed_tests = 0
    failed_tests = 0

    # L·∫∑p qua t·ª´ng d√≤ng trong k·∫øt qu·∫£ cu·ªëi c√πng ƒë·ªÉ ki·ªÉm tra
    for h, row in final_df.iterrows():
        test_name = f"Hash {h}"
        try:
            # 1. L·∫•y th√¥ng tin t·ª´ k·∫øt qu·∫£
            log_source = row['log_source']
            ts_list_result = row['tsList']
            byte_list_result = row['byteList']
            
            # 2. T√¨m l·∫°i group g·ªëc trong DataFrame ngu·ªìn
            original_key = hashmap[h]
            source_df = source_map[log_source]
            group_cols = group_cols_map[log_source]
            
            # D√πng query ƒë·ªÉ l·ªçc ra ch√≠nh x√°c group g·ªëc
            query_str = ' & '.join([f'`{col}` == {repr(val)}' for col, val in zip(group_cols, original_key)])
            original_group = source_df.query(query_str)

            # 3. TH·ª∞C HI·ªÜN C√ÅC B∆Ø·ªöC KI·ªÇM TRA
            
            # Test A: Ki·ªÉm tra s·ªë l∆∞·ª£ng b·∫£n ghi c√≥ kh·ªõp kh√¥ng
            assert len(ts_list_result) == len(original_group), f"S·ªë l∆∞·ª£ng timestamp kh√¥ng kh·ªõp ({len(ts_list_result)} vs {len(original_group)})"
            assert len(byte_list_result) == len(original_group), f"S·ªë l∆∞·ª£ng byte kh√¥ng kh·ªõp ({len(byte_list_result)} vs {len(original_group)})"

            # Test B: Ki·ªÉm tra danh s√°ch timestamp c√≥ kh·ªõp kh√¥ng (s·∫Øp x·∫øp ƒë·ªÉ ch·∫Øc ch·∫Øn)
            ts_list_original = original_group['ts'].tolist()
            assert sorted(ts_list_result) == sorted(ts_list_original), "Danh s√°ch timestamp kh√¥ng kh·ªõp"
            
            # Test C: T·∫°o ra byte_list "chu·∫©n" t·ª´ group g·ªëc v√† so s√°nh
            # ƒê√¢y l√† b√†i test quan tr·ªçng nh·∫•t, x√°c minh logic c·ªßa h√†m .map()
            expected_byte_list = original_group['uid'].map(uid_to_bytes_map).tolist()
            assert byte_list_result == expected_byte_list, "Gi√° tr·ªã ho·∫∑c th·ª© t·ª± c·ªßa byteList kh√¥ng ch√≠nh x√°c"

            print(f"‚úÖ Test PASSED for {test_name} (Source: {log_source}, Key: {original_key})")
            passed_tests += 1

        except AssertionError as e:
            print(f"‚ùå Test FAILED for {test_name} (Source: {log_source}, Key: {original_key})")
            print(f"   L·ªói: {e}")
            failed_tests += 1
        except Exception as e:
            print(f"üí• An unexpected error occurred during test for {test_name}: {e}")
            failed_tests += 1
            
    print("\n--- T·ªïng k·∫øt ki·ªÉm tra ---")
    print(f"üëç S·ªë test th√†nh c√¥ng: {passed_tests}")
    print(f"üëé S·ªë test th·∫•t b·∫°i: {failed_tests}")
    if failed_tests == 0:
        print("\nüéâ Tuy·ªát v·ªùi! T·∫•t c·∫£ d·ªØ li·ªáu ƒë·ªÅu ch√≠nh x√°c.")
    else:
        print("\n‚ö†Ô∏è C√≥ l·ªói x·∫£y ra, vui l√≤ng ki·ªÉm tra l·∫°i logic x·ª≠ l√Ω.")

    return failed_tests == 0

# --- C√ÅCH S·ª¨ D·ª§NG ---
# Gi·∫£ s·ª≠ b·∫°n ƒë√£ ch·∫°y h√†m `aggregate_with_log_source_corrected` v√† c√≥ `final_df`, `hashmap`
# t·ª´ c√°c c√¢u tr·∫£ l·ªùi tr∆∞·ªõc.

# final_df, hashmap = aggregate_with_log_source_corrected(conn_df, http_df, dns_df, ssl_df)

# Ch·∫°y h√†m test
is_correct = test_aggregation_correctness(final_df, hashmap, conn_df, http_df, dns_df, ssl_df)
# ====================== END =======================

# In[8]:


# grouped_df


# In[9]:


# muc do phan bo doi xung quanh trung vi
def calc_bowley_skewness(data):
    if len(data) < 3:
        return 0.0

    q1, q2, q3 = np.percentile(data, [25, 50, 75])

    if (q3 - q1) == 0:
        return 0.0

    bowley_skewness = (q3 + q1 - 2 * q2) / (q3 - q1)
    score = abs(bowley_skewness)
    return score


# In[10]:


# muc do phan tan xung quanh trung vi
def calc_median_absolute_deviation(data):
    if len(data) == 0:
        return 0.0

    mad = np.median(np.abs(data - np.median(data)))
    score = mad
    return score


# In[ ]:


# muc do tuong quan giua cac chu ki
def calc_auto_correlation(data, lag=1):
    if len(data) < lag + 1:
        return 0.0

    n = len(data)
    mean = np.mean(data)
    c_lag = np.sum((data[:n - lag] - mean) * (data[lag:] - mean))
    c0 = np.sum((data - mean) ** 2)

    if c0 == 0:
        return 0.0

    autocorr = c_lag / c0
    score = abs(autocorr)
    return score


# In[12]:


def ts_to_interval(ts_list):
    if len(ts_list) < 2:
        return []

    ts_list_sorted = sorted(ts_list)
    intervals = [(ts_list_sorted[i] - ts_list_sorted[i - 1]) for i in range(1, len(ts_list_sorted))]
    return intervals


# In[ ]:


# calc interval score


# In[ ]:


def calc_timestamp_score(tsList):
    if(type(tsList) is float):
        return 0.0
    if len(tsList) < 2:
        return 0.0

    intervals = ts_to_interval(tsList)
    bowley_skewness = calc_bowley_skewness(intervals)
    mad = calc_median_absolute_deviation(intervals)
    # autocorr = calc_auto_correlation(intervals, lag=1)

    score = (bowley_skewness + mad) / 2
    return score


# In[ ]:


def calc_data_size_score(byteList):
    if(type(byteList) is float):
        return 0.0

    if len(byteList) < 2:
        return 0.0

    bowley_skewness = calc_bowley_skewness(byteList)
    mad = calc_median_absolute_deviation(byteList)
    # autocorr = calc_auto_correlation(byteList, lag=1)

    score = (bowley_skewness + mad) / 2
    return score


# In[14]:

# grouped_df['tsScore'] = grouped_df['tsList'].apply(lambda x: calc_timestamp_score(x))
# grouped_df['dsScore'] = grouped_df['byteList'].apply(lambda x: calc_data_size_score(x))

# grouped_df



#  ================================== new ==================================
HistogramModeSensitivity = 0.05
HistogramBimodalOutlierRemoval = 1
HistogramBimodalMinHoursSeen = 11

def get_frequency_counts(
    connection_histogram: List[int], 
    mode_sensitivity: float
) -> Tuple[Dict[int, int], int, int]:
    """
    Ph√¢n t√≠ch m·ªôt bi·ªÉu ƒë·ªì t·∫ßn su·∫•t ƒë·ªÉ l·∫•y c√°c th√¥ng s·ªë t·ªïng h·ª£p.

    H√†m n√†y ƒë∆∞·ª£c t√°i t·∫°o l·∫°i d·ª±a tr√™n c√°c gi√° tr·ªã tr·∫£ v·ªÅ c·ªßa h√†m c√πng t√™n trong code Go.

    Args:
        connection_histogram: Danh s√°ch t·∫ßn su·∫•t (k·∫øt qu·∫£ c·ªßa histogram).
        mode_sensitivity: (Ch∆∞a s·ª≠ d·ª•ng trong phi√™n b·∫£n n√†y) Ng∆∞·ª°ng ƒë·ªô nh·∫°y. 
                          C√≥ th·ªÉ d√πng ƒë·ªÉ l·ªçc c√°c bin c√≥ gi√° tr·ªã th·∫•p.

    Returns:
        M·ªôt tuple ch·ª©a:
        - freq_count: M·ªôt dictionary ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói m·ª©c t·∫ßn su·∫•t.
        - total_bars: T·ªïng s·ªë "ngƒÉn" (bin) c√≥ ho·∫°t ƒë·ªông (t·∫ßn su·∫•t > 0).
        - longest_run: Chu·ªói d√†i nh·∫•t c√°c "ngƒÉn" c√≥ ho·∫°t ƒë·ªông li√™n ti·∫øp.
    """
    # L·ªçc ra c√°c gi√° tr·ªã t·∫ßn su·∫•t > 0 ƒë·ªÉ t√≠nh to√°n
    active_counts = [count for count in connection_histogram if count > 0]
    
    # 1. freq_count: ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói m·ª©c t·∫ßn su·∫•t.
    # V√≠ d·ª•: [2, 0, 2, 5, 2] -> active_counts l√† [2, 2, 5, 2] -> freq_count l√† {2: 3, 5: 1}
    freq_count = Counter(active_counts)

    # 2. total_bars: T·ªïng s·ªë ngƒÉn c√≥ gi√° tr·ªã > 0.
    total_bars = len(active_counts)

    # 3. longest_run: T√¨m chu·ªói d√†i nh·∫•t c√°c ngƒÉn c√≥ gi√° tr·ªã > 0 li√™n ti·∫øp.
    longest_run = 0
    current_run = 0
    for count in connection_histogram:
        # Trong phi√™n b·∫£n n√†y, ta coi m·ªçi ho·∫°t ƒë·ªông > 0 l√† h·ª£p l·ªá.
        # c√≥ th·ªÉ thay ƒë·ªïi ƒëi·ªÅu ki·ªán th√†nh `count > mode_sensitivity` n·∫øu c·∫ßn.
        if count > 0:
            current_run += 1
        else:
            longest_run = max(longest_run, current_run)
            current_run = 0
    # C·∫≠p nh·∫≠t l·∫ßn cu·ªëi ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªùng h·ª£p chu·ªói k·∫øt th√∫c ·ªü cu·ªëi danh s√°ch
    longest_run = max(longest_run, current_run)

    return freq_count, total_bars, longest_run

def create_histogram(
    bin_edges: List[float], 
    timestamps: List[int], 
    mode_sensitivity: float = 0.0
) -> Tuple[List[int], Dict[int, int], int, int]:
    """
    T·∫°o bi·ªÉu ƒë·ªì t·∫ßn su·∫•t b·∫±ng c√°ch ƒë·∫øm s·ªë l∆∞·ª£ng timestamp r∆°i v√†o m·ªói "ngƒÉn" (bin).

    Args:
        bin_edges: Danh s√°ch c√°c c·∫°nh c·ªßa ngƒÉn. V√≠ d·ª•: [0, 10, 20, 30].
        timestamps: Danh s√°ch c√°c timestamp c·∫ßn ph√¢n lo·∫°i.
        mode_sensitivity: Ng∆∞·ª°ng ƒë·ªô nh·∫°y ƒë·ªÉ chuy·ªÉn cho h√†m ph√¢n t√≠ch con.

    Returns:
        M·ªôt tuple ch·ª©a:
        - connection_histogram: Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t.
        - freq_count: Map ƒë·∫øm t·∫ßn su·∫•t.
        - total_bars: T·ªïng s·ªë ngƒÉn c√≥ ho·∫°t ƒë·ªông.
        - longest_run: Chu·ªói ho·∫°t ƒë·ªông d√†i nh·∫•t.
        
    Raises:
        ValueError: N·∫øu ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá.
    """
    # --- B∆∞·ªõc 1: Ki·ªÉm tra ƒë·∫ßu v√†o ---
    if len(bin_edges) < 2:
        raise ValueError("bin_edges ph·∫£i ch·ª©a √≠t nh·∫•t 2 ph·∫ßn t·ª≠.")
    if not timestamps:
        raise ValueError("timestamps kh√¥ng ƒë∆∞·ª£c r·ªóng.")

    # --- B∆∞·ªõc 2: T·∫°o histogram ---
    # Thay v√¨ d√πng v√≤ng l·∫∑p th·ªß c√¥ng nh∆∞ Go, ta d√πng numpy.histogram.
    # H√†m n√†y c·ª±c k·ª≥ hi·ªáu qu·∫£, nhanh v√† l√† ti√™u chu·∫©n trong Python.
    # N√≥ t·ª± ƒë·ªông x·ª≠ l√Ω vi·ªác s·∫Øp x·∫øp v√† ph√¢n lo·∫°i v√†o c√°c ngƒÉn.
    connection_histogram_np, _ = np.histogram(timestamps, bins=bin_edges)
    
    # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ t·ª´ m·∫£ng numpy v·ªÅ list int ti√™u chu·∫©n c·ªßa Python
    connection_histogram = [int(count) for count in connection_histogram_np]
    
    # --- B∆∞·ªõc 3: L·∫•y c√°c th√¥ng s·ªë t·ªïng h·ª£p ---
    # G·ªçi h√†m tr·ª£ gi√∫p, t∆∞∆°ng t·ª± nh∆∞ c·∫•u tr√∫c c·ªßa code Go
    freq_count, total_bars, longest_run = get_frequency_counts(
        connection_histogram, mode_sensitivity
    )

    return connection_histogram, freq_count, total_bars, longest_run

def compute_histogram_bins(start_time: int, end_time: int, num_bins: int) -> List[float]:
    """
    T·∫°o ra c√°c "c·∫°nh ngƒÉn" (bin edges) ƒë∆∞·ª£c chia ƒë·ªÅu cho m·ªôt bi·ªÉu ƒë·ªì histogram.

    H√†m n√†y nh·∫≠n v√†o m·ªôt kho·∫£ng th·ªùi gian v√† s·ªë l∆∞·ª£ng ngƒÉn mong mu·ªën, sau ƒë√≥ tr·∫£ v·ªÅ
    m·ªôt danh s√°ch c√°c ƒëi·ªÉm th·ªùi gian (d∆∞·ªõi d·∫°ng float) chia ƒë·ªÅu kho·∫£ng th·ªùi gian ƒë√≥.

    Args:
        start_time: Timestamp b·∫Øt ƒë·∫ßu (d∆∞·ªõi d·∫°ng Unix timestamp, ki·ªÉu int).
        end_time: Timestamp k·∫øt th√∫c (d∆∞·ªõi d·∫°ng Unix timestamp, ki·ªÉu int).
        num_bins: S·ªë l∆∞·ª£ng "ngƒÉn" (bin) mong mu·ªën.

    Returns:
        M·ªôt danh s√°ch g·ªìm `num_bins + 1` c·∫°nh ngƒÉn (d∆∞·ªõi d·∫°ng float).

    Raises:
        ValueError: N·∫øu c√°c tham s·ªë ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá.
    """
    # --- B∆∞·ªõc 1: Ki·ªÉm tra ƒë·∫ßu v√†o ---
    if num_bins <= 0:
        raise ValueError("S·ªë l∆∞·ª£ng ngƒÉn (num_bins) ph·∫£i l·ªõn h∆°n 0.")
    
    if end_time <= start_time:
        raise ValueError("Kho·∫£ng th·ªùi gian kh√¥ng h·ª£p l·ªá: end_time ph·∫£i l·ªõn h∆°n start_time.")

    # --- B∆∞·ªõc 2: T√≠nh to√°n c√°c c·∫°nh ngƒÉn b·∫±ng NumPy ---
    # H√†m numpy.linspace l√† c√¥ng c·ª• ho√†n h·∫£o cho vi·ªác n√†y.
    # N√≥ t·∫°o ra m·ªôt chu·ªói c√°c s·ªë ƒë∆∞·ª£c chia ƒë·ªÅu trong m·ªôt kho·∫£ng cho tr∆∞·ªõc.
    # ƒê·ªÉ c√≥ N ngƒÉn, ch√∫ng ta c·∫ßn N + 1 c·∫°nh.
    edge_count = num_bins + 1
    
    # np.linspace(start, stop, num) s·∫Ω t·∫°o ra `num` ƒëi·ªÉm t·ª´ `start` ƒë·∫øn `stop`.
    bin_edges_np = np.linspace(start_time, end_time, num=edge_count)
    
    # Chuy·ªÉn ƒë·ªïi t·ª´ m·∫£ng numpy sang list float ti√™u chu·∫©n c·ªßa Python ƒë·ªÉ gi·ªëng v·ªõi
    # ki·ªÉu tr·∫£ v·ªÅ `[]float64` c·ªßa Go.
    bin_edges = bin_edges_np.tolist()

    return bin_edges


def get_histogram_score(dataset, datasetMax, tsList, modeSensitivity, bimodalOutlierRemoval,bimodalMinHoursSeen,beaconTimeSpan):
    binEdges = compute_histogram_bins(datasetMin, datasetMax, beaconTimeSpan) # TODO
    freqList, freqCount, totalBars, longestRun = create_histogram(binEdges, tsList, modeSensitivity) 
    svscore = calculate_coefficient_of_variation_score_final(freqList)
    # bitmodel
    bitmodalFitScore = calculate_bimodal_fit_score(freqCount, totalBars, longestRun, bimodalOutlierRemoval, bimodalMinHoursSeen)
    score = math.Max(cvScore, bimodalFitScore)
    return score


def GetBeaconMinMaxTimestamps():
    # l·∫•y timestamp max trong data
    # l·∫•y timestamp max - 24h
    # if timestamp max - timestamp min < 24h th√¨ l·∫•y timestamp min
    # ng∆∞·ª£c l·∫°i th√¨ l·∫•y timestamp max - 24h
    return minTS, maxTS

minTSBeacon, maxTSBeacon = GetBeaconMinMaxTimestamps()

get_histogram_score(
    minTSBeacon,
    maxTSBeacon,
    TSList,
    HistogramModeSensitivity, # 0.05
    HistogramBimodalOutlierRemoval, # 1
    HistogramBimodalMinHoursSeen, # 11
    24 # 24 hours span for beacon detection
)
# ================================== end ==================================