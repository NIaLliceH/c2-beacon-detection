# INTEGRATE FILEBEAT
input {
	beats {
		port => 5044
	}

	# tcp {
	# 	port => 50000
	# }
}

filter {
  # Chỉ xử lý các sự kiện flow của Suricata hoặc conn của Zeek
  if ("suricata.eve.flow" in [tags]) or ([event][dataset] == "zeek.conn") {

    # Chuẩn hóa các trường IP và bytes
    if "suricata.eve.flow" in [tags] {
      mutate {
        rename => {
          "[source][ip]" => "source_ip"
          "[destination][ip]" => "destination_ip"
          "[suricata][eve][flow][bytes_toserver]" => "source_bytes"
          "[suricata][eve][flow][bytes_toclient]" => "destination_bytes"
        }
      }
    } else if [event][dataset] == "zeek.conn" {
      mutate {
        rename => {
          "[source][ip]" => "source_ip"
          "[destination][ip]" => "destination_ip"
          "[source][bytes]" => "source_bytes"
          "[destination][bytes]" => "destination_bytes"
        }
      }
    }

    # Tạo một ID duy nhất, có hướng cho cặp kết nối
    mutate {
      add_field => { "connection_pair_id" => "%{source_ip}-%{destination_ip}" }
    }

    aggregate {
      task_id => "%{connection_pair_id}"
      code => "
        map['connection_count'] ||= 0
        map['connection_count'] += 1
        map['source_ip'] ||= event.get('source_ip')
        map['destination_ip'] ||= event.get('destination_ip')

        # Initialize arrays
        map['timestamps'] ||= []
        map['timestamps'] << event.get('@timestamp').to_i

        map['source_bytes'] ||= []
        map['source_bytes'] << (event.get('source_bytes') || 0)

        map['destination_bytes'] ||= []
        map['destination_bytes'] << (event.get('destination_bytes') || 0)

        # Cancel original event
        event.cancel()
"
      push_map_as_event_on_timeout => true
      timeout => 3600 # Timeout cứng 1 giờ
      inactivity_timeout => 600 # Timeout 10 phút không hoạt động
      timeout_task_id_field => "connection_pair"
      timeout_code => "
        # Tính toán khoảng thời gian giữa các timestamp liên tiếp
        intervals = []
        if event.get('timestamps') && event.get('timestamps').length > 1
          sorted_ts = event.get('timestamps').sort
          (1...sorted_ts.length).each { |i| intervals << sorted_ts[i] - sorted_ts[i-1] }
        end
        event.set('beacon_intervals', intervals)

        # Tính toán các chỉ số thống kê
        if intervals.any?
          sum = intervals.inject(0){|sum,x| sum + x }
          mean = sum / intervals.length.to_f
          event.set('beacon_avg_interval', mean)

          # Tính Jitter (Độ lệch chuẩn - Standard Deviation)
          sum_sq_diff = intervals.map { |x| (x - mean) ** 2 }.inject(0, :+)
          std_dev = Math.sqrt(sum_sq_diff / intervals.length.to_f)
          event.set('beacon_jitter', std_dev)
        else
          event.set('beacon_avg_interval', 0)
          event.set('beacon_jitter', 0)
        end
        
        # Tính toán thống kê payload
        if event.get('source_bytes') && event.get('source_bytes').any?
            event.set('avg_source_bytes', event.get('source_bytes').sum / event.get('source_bytes').length.to_f)
        end
        if event.get('destination_bytes') && event.get('destination_bytes').any?
            event.set('avg_destination_bytes', event.get('destination_bytes').sum / event.get('destination_bytes').length.to_f)
        end
        
        event.remove('timestamps')
        event.remove('source_bytes')
        event.remove('destination_bytes')
      "
      timeout_tags => ['_aggregate_timeout']
    }
  }
}



output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "logstash_internal"
    password => "logstash"
    # index => "filebeat-%{+YYYY.MM.dd}"
    data_stream => "true"
    data_stream_dataset => "suricata"
    data_stream_namespace => "default"
  }

  stdout {
    codec => rubydebug
  }
}


###########
# Custom logstash.conf for PCAP C2 Analysis Pipeline
# input {
#   file {
#     path => "/usr/share/logstash/logstash-input/traffic-analyse.json"
#     start_position => "beginning"
#     sincedb_path => "/usr/share/logstash/data/sincedb_c2"
#     mode => "tail"
#     codec => "json"
#   }
# }

# filter {
#   # The 'json' codec in the input block handles parsing.
#   # Additional filters can be added here if needed, e.g., for data enrichment.
  
#   mutate {
#     # Convert string fields to appropriate numeric types if not handled by the template
#     convert => {
#       "packet_count" => "integer"
#       "total_payload_size" => "integer"
#       "connection_duration" => "float"
#       "connection_interval" => "float"
#       "average_jitter" => "float"
#     }
#   }
  
#   date {
#     match => ["timestamp", "ISO8601"]
#     target => "@timestamp"
#   }
# }

# output {
#   elasticsearch {
#     hosts => ["http://elasticsearch:9200"]
#     data_stream => "true"
#     # index => "pcap-c2-analysis"
#     data_stream_type => "logs"
#     data_stream_dataset => "pcap_c2"
#     data_stream_namespace => "default"
#     user => "logstash_internal"
#     password => "logstash"
#     # password => "${LOGSTASH_INTERNAL_PASSWORD}"
#   }
  
#   # Optional: Output to stdout for debugging purposes
#   stdout {
#     codec => rubydebug
#   }
# }